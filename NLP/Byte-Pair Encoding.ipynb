{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN8D9rB5P+qhuOQCrAnOw4u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### **Tokenización por Palabras**\n","\n","\n"],"metadata":{"id":"FHI8gDVtaMV2"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"GK9hK-TrZm7P","executionInfo":{"status":"ok","timestamp":1695411566392,"user_tz":180,"elapsed":7,"user":{"displayName":"Tomás Odriozola","userId":"17069844314933857607"}}},"outputs":[],"source":["corpus = [\n","    \"This is the Hugging Face Course.\",\n","    \"This chapter is about tokenization.\",\n","    \"This section shows several tokenizer algorithms.\",\n","    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n","]"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"ZT9gZB2tZtzC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"],"metadata":{"id":"2umPuIFVaD8v","executionInfo":{"status":"ok","timestamp":1695411620654,"user_tz":180,"elapsed":1016,"user":{"displayName":"Tomás Odriozola","userId":"17069844314933857607"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","\n","#Genera frecuencia de palabras\n","word_freqs = defaultdict(int)\n","for text in corpus:\n","    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n","    new_words = [word for word, offset in words_with_offsets]\n","    for word in new_words:\n","        word_freqs[word] += 1\n","\n","print(word_freqs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edJo3zJGZ4vW","executionInfo":{"status":"ok","timestamp":1695412601998,"user_tz":180,"elapsed":5,"user":{"displayName":"Tomás Odriozola","userId":"17069844314933857607"}},"outputId":"6b340c3d-4708-49ac-f4c8-489d0274bfd2"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["defaultdict(<class 'int'>, {'This': 3, 'Ġis': 2, 'Ġthe': 1, 'ĠHugging': 1, 'ĠFace': 1, 'ĠCourse': 1, '.': 4, 'Ġchapter': 1, 'Ġabout': 1, 'Ġtokenization': 1, 'Ġsection': 1, 'Ġshows': 1, 'Ġseveral': 1, 'Ġtokenizer': 1, 'Ġalgorithms': 1, 'Hopefully': 1, ',': 1, 'Ġyou': 1, 'Ġwill': 1, 'Ġbe': 1, 'Ġable': 1, 'Ġto': 1, 'Ġunderstand': 1, 'Ġhow': 1, 'Ġthey': 1, 'Ġare': 1, 'Ġtrained': 1, 'Ġand': 1, 'Ġgenerate': 1, 'Ġtokens': 1})\n"]}]},{"cell_type":"code","source":["#Creo Vocabulario de tokens utilizado\n","alphabet = []\n","for word in word_freqs.keys():\n","    for letter in word:\n","        if letter not in alphabet:\n","            alphabet.append(letter)\n","alphabet.sort()\n","vocab = [\"<|endoftext|>\"] + alphabet.copy()\n","\n","print(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CxoAwAERagrO","executionInfo":{"status":"ok","timestamp":1695412599789,"user_tz":180,"elapsed":461,"user":{"displayName":"Tomás Odriozola","userId":"17069844314933857607"}},"outputId":"21230b8b-be6c-45b9-d1fa-20af7af9a137"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ġ']\n"]}]},{"cell_type":"code","source":["#Recibe todos los caracteres separados de las palabras de los documentos, los empareja y calcula la frecuencia de los emparejamientos\n","def compute_pair_freqs(splits):\n","    pair_freqs = defaultdict(int)\n","    for word, freq in word_freqs.items():\n","        split = splits[word]\n","        if len(split) == 1:\n","            continue\n","        for i in range(len(split) - 1):\n","            pair = (split[i], split[i + 1])\n","            pair_freqs[pair] += freq\n","    return pair_freqs\n","\n","splits = {word: [c for c in word] for word in word_freqs.keys()}\n","pair_freqs = compute_pair_freqs(splits)\n","\n","for i, key in enumerate(pair_freqs.keys()):\n","    print(f\"{key}: {pair_freqs[key]}\")\n","    if i >= 5:\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4RAsg_oNbCsK","executionInfo":{"status":"ok","timestamp":1695412596941,"user_tz":180,"elapsed":364,"user":{"displayName":"Tomás Odriozola","userId":"17069844314933857607"}},"outputId":"292444b3-9c99-400d-bd7d-2ada3064b9e3"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["('T', 'h'): 3\n","('h', 'i'): 3\n","('i', 's'): 5\n","('Ġ', 'i'): 2\n","('Ġ', 't'): 7\n","('t', 'h'): 3\n"]}]},{"cell_type":"code","source":["def merge_pair(a, b, splits):\n","    for word in word_freqs:\n","        split = splits[word]\n","        if len(split) == 1:\n","            continue\n","\n","        i = 0\n","        while i < len(split) - 1:\n","            if split[i] == a and split[i + 1] == b:\n","                split = split[:i] + [a + b] + split[i + 2 :]\n","            else:\n","                i += 1\n","        splits[word] = split\n","    return splits\n","\n","splits = merge_pair(\"Ġ\", \"t\", splits)\n","print(splits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ptSj5b6ab30N","executionInfo":{"status":"ok","timestamp":1695412123158,"user_tz":180,"elapsed":4,"user":{"displayName":"Tomás Odriozola","userId":"17069844314933857607"}},"outputId":"911471c0-8b59-42cc-8660-d55217e35195"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["{'This': ['T', 'h', 'i', 's'], 'Ġis': ['Ġ', 'i', 's'], 'Ġthe': ['Ġt', 'h', 'e'], 'ĠHugging': ['Ġ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'], 'ĠFace': ['Ġ', 'F', 'a', 'c', 'e'], 'ĠCourse': ['Ġ', 'C', 'o', 'u', 'r', 's', 'e'], '.': ['.'], 'Ġchapter': ['Ġ', 'c', 'h', 'a', 'p', 't', 'e', 'r'], 'Ġabout': ['Ġ', 'a', 'b', 'o', 'u', 't'], 'Ġtokenization': ['Ġt', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n'], 'Ġsection': ['Ġ', 's', 'e', 'c', 't', 'i', 'o', 'n'], 'Ġshows': ['Ġ', 's', 'h', 'o', 'w', 's'], 'Ġseveral': ['Ġ', 's', 'e', 'v', 'e', 'r', 'a', 'l'], 'Ġtokenizer': ['Ġt', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'], 'Ġalgorithms': ['Ġ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'], 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'], ',': [','], 'Ġyou': ['Ġ', 'y', 'o', 'u'], 'Ġwill': ['Ġ', 'w', 'i', 'l', 'l'], 'Ġbe': ['Ġ', 'b', 'e'], 'Ġable': ['Ġ', 'a', 'b', 'l', 'e'], 'Ġto': ['Ġt', 'o'], 'Ġunderstand': ['Ġ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'], 'Ġhow': ['Ġ', 'h', 'o', 'w'], 'Ġthey': ['Ġt', 'h', 'e', 'y'], 'Ġare': ['Ġ', 'a', 'r', 'e'], 'Ġtrained': ['Ġt', 'r', 'a', 'i', 'n', 'e', 'd'], 'Ġand': ['Ġ', 'a', 'n', 'd'], 'Ġgenerate': ['Ġ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'], 'Ġtokens': ['Ġt', 'o', 'k', 'e', 'n', 's']}\n"]}]},{"cell_type":"markdown","source":["### **Byte-Pair Encoding**"],"metadata":{"id":"svP4R-z5cYlq"}},{"cell_type":"code","source":["merges = {(\"Ġ\", \"t\"): \"Ġt\"}\n","vocab.append(\"Ġt\")\n","\n","vocab_size = 50\n","\n","while len(vocab) < vocab_size:\n","    pair_freqs = compute_pair_freqs(splits)\n","    best_pair = \"\"\n","    max_freq = None\n","    for pair, freq in pair_freqs.items():\n","        if max_freq is None or max_freq < freq:\n","            best_pair = pair\n","            max_freq = freq\n","    splits = merge_pair(*best_pair, splits)\n","    merges[best_pair] = best_pair[0] + best_pair[1]\n","    vocab.append(best_pair[0] + best_pair[1])\n","\n","print(merges)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v5Zlu0xrcbN9","executionInfo":{"status":"ok","timestamp":1695412606187,"user_tz":180,"elapsed":409,"user":{"displayName":"Tomás Odriozola","userId":"17069844314933857607"}},"outputId":"f5bffa42-1db6-4d24-844a-76a4cf1e0594"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["{('Ġ', 't'): 'Ġt', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ġ', 'a'): 'Ġa', ('Ġt', 'o'): 'Ġto', ('e', 'n'): 'en', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ġto', 'k'): 'Ġtok', ('Ġtok', 'en'): 'Ġtoken', ('n', 'd'): 'nd', ('Ġ', 'is'): 'Ġis', ('Ġt', 'h'): 'Ġth', ('Ġth', 'e'): 'Ġthe', ('i', 'n'): 'in', ('Ġa', 'b'): 'Ġab'}\n"]}]}]}